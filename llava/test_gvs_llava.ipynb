{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wangz3/miniconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-26 21:38:11,503] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ViTMAEConfig()\n",
    "# LlavaGeoConfig()\n",
    "from llava.model.language_model.llava_llama import LlavaGeoConfig, LlavaGeoLlamaForCausalLM\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from transformers.models.vit_mae.configuration_vit_mae import *\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                         LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_geo. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# init model from scratch\n",
    "config = LlavaGeoConfig.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "model = LlavaGeoLlamaForCausalLM(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_geo. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.56s/it]\n",
      "Some weights of LlavaGeoLlamaForCausalLM were not initialized from the model checkpoint at liuhaotian/llava-v1.5-7b and are newly initialized: ['mae_decoder.decoder_layers.1.intermediate.dense.weight', 'mae_decoder.decoder_layers.0.intermediate.dense.weight', 'mae_decoder.mask_token', 'mae_decoder.decoder_layers.1.layernorm_before.bias', 'mae_decoder.decoder_layers.0.attention.output.dense.weight', 'mae_decoder.decoder_layers.0.attention.attention.key.bias', 'mae_decoder.decoder_norm.weight', 'mae_decoder.decoder_layers.1.attention.attention.value.bias', 'mae_decoder.decoder_layers.0.layernorm_after.bias', 'mae_decoder.decoder_layers.0.layernorm_after.weight', 'mae_decoder.decoder_layers.1.attention.attention.query.bias', 'mae_decoder.decoder_embed.weight', 'mae_decoder.decoder_layers.0.attention.attention.query.bias', 'mae_decoder.decoder_layers.0.layernorm_before.bias', 'mae_decoder.decoder_layers.1.attention.output.dense.weight', 'mae_decoder.decoder_embed.bias', 'mae_decoder.decoder_layers.0.attention.attention.value.bias', 'mae_decoder.decoder_layers.1.layernorm_after.weight', 'mae_decoder.decoder_layers.1.attention.attention.value.weight', 'mae_decoder.decoder_layers.1.intermediate.dense.bias', 'mae_decoder.decoder_layers.0.attention.attention.key.weight', 'mae_decoder.decoder_layers.0.output.dense.bias', 'mae_decoder.decoder_norm.bias', 'mae_decoder.decoder_layers.1.attention.attention.query.weight', 'mae_decoder.decoder_layers.0.output.dense.weight', 'mae_decoder.decoder_pred.bias', 'mae_decoder.decoder_layers.0.attention.attention.value.weight', 'mae_decoder.decoder_pos_embed', 'mae_decoder.decoder_layers.1.attention.attention.key.bias', 'mae_decoder.decoder_layers.0.attention.attention.query.weight', 'mae_decoder.decoder_layers.0.attention.output.dense.bias', 'mae_decoder.decoder_pred.weight', 'mae_decoder.decoder_layers.1.output.dense.weight', 'mae_decoder.decoder_layers.1.layernorm_after.bias', 'mae_decoder.decoder_layers.1.layernorm_before.weight', 'mae_decoder.decoder_layers.1.attention.output.dense.bias', 'mae_decoder.decoder_layers.1.output.dense.bias', 'mae_decoder.decoder_layers.1.attention.attention.key.weight', 'mae_decoder.decoder_layers.0.layernorm_before.weight', 'mae_decoder.decoder_layers.0.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlavaGeoLlamaForCausalLM(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (vision_tower): CLIPVisionTower()\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  (mae_decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-1): 2 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=588, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from pretraing\n",
    "mae_decoder_config = {\n",
    "    \"base_config\": \"facebook/vit-mae-base\",\n",
    "    \"norm_pix_loss\": True,\n",
    "    \"decoder_num_hidden_layers\": 2\n",
    "}\n",
    "losses = [\"lm\", \"mae\"]\n",
    "loss_weights = {\n",
    "    \"lm\": 1.0,\n",
    "    \"mae\": 1.0\n",
    "}\n",
    "device = 'cuda:2'\n",
    "\n",
    "kwargs = {\n",
    "    \"device_map\": device,\n",
    "    \"mae_decoder_config\": mae_decoder_config,\n",
    "    \"losses\": losses,\n",
    "    \"loss_weights\": loss_weights,\n",
    "    # \"torch_dtype\": torch.bfloat16\n",
    "}\n",
    "\n",
    "model = LlavaGeoLlamaForCausalLM.from_pretrained('liuhaotian/llava-v1.5-7b', **kwargs)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config\n",
    "# Check dtype of each parameter\n",
    "for name, param in model.named_parameters():\n",
    "    param_dtype = param.dtype\n",
    "    print(name, param_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = 'cuda:2'\n",
    "\n",
    "vision_tower_dtype = torch.bfloat16\n",
    "# vision_tower_dtype = torch.float32\n",
    "# model.to(device, dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "\n",
    "vision_tower = model.get_vision_tower()\n",
    "if not vision_tower.is_loaded:\n",
    "    vision_tower.load_model()\n",
    "vision_tower.to(device=device, dtype=vision_tower_dtype)\n",
    "image_processor = vision_tower.image_processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== test forward ===\n",
      "images.shape: torch.Size([1, 3, 336, 336])\n",
      "image_features.shape: torch.Size([1, 577, 1024])\n",
      "inputs_embeds shape torch.Size([1, 586, 4096])\n",
      "image_features_with_cls shape torch.Size([1, 577, 4096])\n",
      "lm loss: tensor(4.1562, device='cuda:2', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "sequence_unmasked shape torch.Size([1, 144, 4096])\n",
      "mask shape torch.Size([1, 576])\n",
      "ids_restore shape torch.Size([1, 576])\n",
      "reconstruction_logits shape torch.Size([1, 576, 588])\n",
      "pred shape | target shape: torch.Size([1, 576, 588]) torch.Size([1, 576, 588])\n",
      "reconstruction_loss tensor(nan, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(nan, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "> \u001b[0;32m/data/wangz3/projects/ecole-gvs-method/third_party/LLaVA/llava/model/language_model/llava_llama.py\u001b[0m(385)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    383 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    384 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 385 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    386 \u001b[0;31m            \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    387 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([[[-0.0903,  0.2314,  0.0400,  ...,  0.5352, -0.2451,  0.3047],\n",
      "         [ 0.4766,  0.1689, -0.1875,  ...,  0.3613, -0.0610,  0.6406],\n",
      "         [-0.0723,  0.7305, -0.3438,  ..., -0.1123, -0.1260, -0.0031],\n",
      "         ...,\n",
      "         [ 1.2422, -0.0046, -1.0391,  ...,  0.2637,  0.4980,  0.6367],\n",
      "         [-0.1318,  0.2695, -1.0312,  ..., -0.1953,  0.2852, -0.0378],\n",
      "         [-0.2285,  0.4102, -0.5117,  ..., -0.0850,  0.5586,  0.0559]]],\n",
      "       device='cuda:2', dtype=torch.bfloat16, grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get inputs\n",
    "text_prompt = DEFAULT_IMAGE_TOKEN + '\\n' + \"What's in this image?\"\n",
    "input_ids = tokenizer_image_token(\n",
    "    text_prompt, \n",
    "    tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device=model.device)\n",
    "\n",
    "image = Image.open('/data/wangz3/projects/ecole-gvs-method/third_party/LLaVA/images/llava_logo.png').convert('RGB')\n",
    "image_tensor = process_images([image], image_processor, {})\n",
    "if type(image_tensor) is list:\n",
    "    image_tensor = [image.to(model.device, dtype=vision_tower_dtype) for image in image_tensor]\n",
    "else:\n",
    "    image_tensor = image_tensor.to(model.device, dtype=vision_tower_dtype)\n",
    "\n",
    "# do inference\n",
    "conv_mode = \"llava_v1\"\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "\n",
    "# print(\"=== test inference ===\")\n",
    "# temperature = 0.2\n",
    "# output_ids = model.generate(\n",
    "#     input_ids,\n",
    "#     images=image_tensor,\n",
    "#     do_sample=True if temperature > 0 else False,\n",
    "#     temperature=temperature,\n",
    "#     max_new_tokens=520,\n",
    "#     # streamer=streamer,\n",
    "#     use_cache=True,\n",
    "#     stopping_criteria=[stopping_criteria])\n",
    "# outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "# print(outputs)\n",
    "\n",
    "\n",
    "print(\"=== test forward ===\")\n",
    "labels = input_ids.clone()\n",
    "# image_tensor = torch.rand(1, 3, 336, 336).to(device=model.device, dtype=torch.float16)\n",
    "outputs = model(input_ids, labels = input_ids, images=image_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "select feature: cls_patch\n",
      "images.shape: torch.Size([1, 3, 336, 336])\n",
      "image_features.shape: torch.Size([1, 577, 1024])\n",
      "select feature: patch\n",
      "inputs_embeds shape torch.Size([1, 586, 4096])\n",
      "image_features_with_cls shape torch.Size([1, 577, 4096])\n",
      "sequence_unmasked shape torch.Size([1, 144, 4096])\n",
      "mask shape torch.Size([1, 576])\n",
      "ids_restore shape torch.Size([1, 576])\n",
      "reconstruction_logits shape torch.Size([1, 576, 588])\n",
      "pred shape | target shape: torch.Size([1, 576, 588]) torch.Size([1, 576, 588])\n",
      "reconstruction_loss tensor(nan, device='cuda:2', grad_fn=<DivBackward0>)\n",
      "total loss: tensor(nan, device='cuda:2', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# device = torch.device(\"cuda:7\")\n",
    "# # print(text_inputs)\n",
    "# model.to(device, dtype=torch.float16)\n",
    "# vision_tower.to(device=model.device, dtype=torch.float16)\n",
    "# model.eval()\n",
    "# print(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "text_prompt = DEFAULT_IMAGE_TOKEN + '\\n' + \"Hello, my dog is cute\"\n",
    "input_ids = tokenizer_image_token(\n",
    "    text_prompt, \n",
    "    tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device=model.device)\n",
    "print(input_ids.shape)\n",
    "images = torch.rand(1, 3, 336, 336).to(device=model.device, dtype=torch.float16)\n",
    "# text_inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model(input_ids, labels = input_ids, images=images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def count_parameters(model):\n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     print(f'{total_params:,} total parameters.')\n",
    "#     llava_params = sum(p.numel() for p in model.model.parameters())\n",
    "#     print(f'{llava_params:,} llava parameters.')\n",
    "#     mm_projector_params = sum(p.numel() for p in model.model.mm_projector.parameters())\n",
    "#     print(f'{mm_projector_params:,} mm projector parameters.')\n",
    "#     mae_decoder_params = sum(p.numel() for p in model.mae_decoder.parameters())\n",
    "#     print(f'{mae_decoder_params:,} mae decoder parameters.')\n",
    "\n",
    "#     print(\"mae decoder param ratio:\", mae_decoder_params / total_params)\n",
    "#     # trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "# print(model)\n",
    "# count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device, dtype=torch.float16)\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer {name} has dtype: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTMAEConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTMAEForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 4,\n",
      "  \"decoder_intermediate_size\": 8,\n",
      "  \"decoder_num_attention_heads\": 2,\n",
      "  \"decoder_num_hidden_layers\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 4,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.75,\n",
      "  \"model_type\": \"vit_mae\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from transformers.models.vit_mae.configuration_vit_mae import *\n",
    "# from transformers.models.vit_mae.configuration_vit_mae import VitMAEConfig\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "# mae_model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
    "config = ViTMAEConfig.from_pretrained(\"facebook/vit-mae-base\")\n",
    "config.hidden_size = 4\n",
    "config.decoder_hidden_size = 4\n",
    "config.decoder_intermediate_size = 8\n",
    "config.intermediate_size = 8\n",
    "config.decoder_num_attention_heads = 2\n",
    "config.num_attention_heads = 2\n",
    "config.num_hidden_layers = 2\n",
    "config.decoder_num_hidden_layers = 2\n",
    "\n",
    "print(config)\n",
    "\n",
    "from transformers.models.vit_mae.modeling_vit_mae import *\n",
    "mae_decoder = ViTMAEDecoder(config, num_patches = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "         [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "         [0.4556, 0.6323, 0.3489, 0.4017],\n",
      "         [0.0223, 0.1689, 0.2939, 0.5185],\n",
      "         [0.6977, 0.8000, 0.1610, 0.2823],\n",
      "         [0.6816, 0.9152, 0.3971, 0.8742],\n",
      "         [0.4194, 0.5529, 0.9527, 0.0362],\n",
      "         [0.1852, 0.3734, 0.3051, 0.9320],\n",
      "         [0.1759, 0.2698, 0.1507, 0.0317],\n",
      "         [0.2081, 0.9298, 0.7231, 0.7423],\n",
      "         [0.5263, 0.2437, 0.5846, 0.0332],\n",
      "         [0.1387, 0.2422, 0.8155, 0.7932],\n",
      "         [0.2783, 0.4820, 0.8198, 0.9971],\n",
      "         [0.6984, 0.5675, 0.8352, 0.2056],\n",
      "         [0.5932, 0.1123, 0.1535, 0.2417],\n",
      "         [0.7262, 0.7011, 0.2038, 0.6511]]])\n",
      "tensor([[0.7745, 0.4369, 0.5191, 0.6159, 0.8102, 0.9801, 0.1147, 0.3168, 0.6965,\n",
      "         0.9143, 0.9351, 0.9412, 0.5995, 0.0652, 0.5460, 0.1872]])\n",
      "tensor([[13,  6, 15,  7,  1,  2, 14, 12,  3,  8,  0,  4,  9, 10, 11,  5]])\n",
      "tensor([[10,  4,  5,  8, 11, 15,  1,  3,  9, 12, 13, 14,  7,  0,  6,  2]])\n",
      "tensor([[[13, 13, 13, 13],\n",
      "         [ 6,  6,  6,  6],\n",
      "         [15, 15, 15, 15],\n",
      "         [ 7,  7,  7,  7],\n",
      "         [ 1,  1,  1,  1],\n",
      "         [ 2,  2,  2,  2],\n",
      "         [14, 14, 14, 14],\n",
      "         [12, 12, 12, 12]]])\n",
      "tensor([[[0.6984, 0.5675, 0.8352, 0.2056],\n",
      "         [0.4194, 0.5529, 0.9527, 0.0362],\n",
      "         [0.7262, 0.7011, 0.2038, 0.6511],\n",
      "         [0.1852, 0.3734, 0.3051, 0.9320],\n",
      "         [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "         [0.4556, 0.6323, 0.3489, 0.4017],\n",
      "         [0.5932, 0.1123, 0.1535, 0.2417],\n",
      "         [0.2783, 0.4820, 0.8198, 0.9971]]])\n",
      "tensor([[1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.]])\n",
      "torch.Size([1, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "def random_masking(mask_ratio, sequence, noise=None):\n",
    "    \"\"\"\n",
    "    Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random\n",
    "    noise.\n",
    "\n",
    "    Args:\n",
    "        sequence (`torch.LongTensor` of shape `(batch_size, sequence_length, dim)`)\n",
    "        noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is\n",
    "            mainly used for testing purposes to control randomness and maintain the reproducibility\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, dim = sequence.shape\n",
    "    len_keep = int(seq_length * (1 - mask_ratio))\n",
    "\n",
    "    if noise is None:\n",
    "        noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]\n",
    "    print(sequence)\n",
    "    print(noise)\n",
    "\n",
    "    # sort noise for each sample\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "    print(ids_shuffle)\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "    print(ids_restore)\n",
    "\n",
    "    # keep the first subset\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "    sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n",
    "    print(ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n",
    "    print(sequence_unmasked)\n",
    "\n",
    "    # generate the binary mask: 0 is keep, 1 is remove\n",
    "    mask = torch.ones([batch_size, seq_length], device=sequence.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    # unshuffle to get the binary mask\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "    print(mask)\n",
    "    return sequence_unmasked, mask, ids_restore\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "batch_size, num_patches, dim = 1, 16, config.hidden_size \n",
    "test_seq = torch.rand((batch_size, num_patches, dim))\n",
    "sequence_unmasked, mask, ids_restore = random_masking(0.5, test_seq)\n",
    "# add cls token\n",
    "cls_token = torch.rand((batch_size, 1, dim))\n",
    "hidden_states = torch.cat((cls_token, sequence_unmasked), dim=1)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 4])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "decoder_output = mae_decoder(hidden_states, ids_restore)\n",
    "print(hidden_states.shape)\n",
    "print(ids_restore.shape)\n",
    "print(decoder_output.logits.shape) # (batch_size, num_patches, patch_size * patch_size * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 32,\n",
      "  \"projection_dim\": 512,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "CLIPVisionEmbeddings(\n",
      "  (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "  (position_embedding): Embedding(50, 768)\n",
      ")\n",
      "patch_embeds.shape torch.Size([1, 768, 7, 7])\n",
      "patch_embeds.shape falttened torch.Size([1, 49, 768])\n",
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# the following is how CLIP encoder get the image patches\n",
    "# the patches are ordered: left to right, top to bottom\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionConfig\n",
    "class CLIPVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches + 1\n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        target_dtype = self.patch_embedding.weight.dtype\n",
    "        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n",
    "        print(\"patch_embeds.shape\", patch_embeds.shape)\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n",
    "        print(\"patch_embeds.shape falttened\", patch_embeds.shape)\n",
    "\n",
    "        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n",
    "        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        return embeddings\n",
    "\n",
    "clip_config = CLIPVisionConfig()\n",
    "print(clip_config)\n",
    "clip_emb_layer = CLIPVisionEmbeddings(clip_config)\n",
    "print(clip_emb_layer)\n",
    "pixel_values = torch.rand((batch_size, 3, 224, 224))\n",
    "patch_embeds = clip_emb_layer(pixel_values)\n",
    "print(patch_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([1, 3, 7, 32, 7, 32])\n",
      "torch.Size([1, 7, 7, 32, 32, 3])\n",
      "torch.Size([1, 49, 3072])\n",
      "torch.Size([1, 49, 3072])\n"
     ]
    }
   ],
   "source": [
    "patch_size, num_channels = 32, 3\n",
    "def patchify(pixel_values):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values.\n",
    "\n",
    "    Returns:\n",
    "        `torch.FloatTensor` of shape `(batch_size, num_patches, patch_size**2 * num_channels)`:\n",
    "            Patchified pixel values.\n",
    "    \"\"\"\n",
    "    # sanity checks\n",
    "    if (pixel_values.shape[2] != pixel_values.shape[3]) or (pixel_values.shape[2] % patch_size != 0):\n",
    "        raise ValueError(\"Make sure the pixel values have a squared size that is divisible by the patch size\")\n",
    "    if pixel_values.shape[1] != num_channels:\n",
    "        raise ValueError(\n",
    "            \"Make sure the number of channels of the pixel values is equal to the one set in the configuration\"\n",
    "        )\n",
    "\n",
    "    # patchify\n",
    "    batch_size = pixel_values.shape[0]\n",
    "    num_patches_one_direction = pixel_values.shape[2] // patch_size\n",
    "    print(num_patches_one_direction)\n",
    "    patchified_pixel_values = pixel_values.reshape(\n",
    "        batch_size, num_channels, num_patches_one_direction, patch_size, num_patches_one_direction, patch_size\n",
    "    )\n",
    "    print(patchified_pixel_values.shape)\n",
    "    patchified_pixel_values = torch.einsum(\"nchpwq->nhwpqc\", patchified_pixel_values)\n",
    "    print(patchified_pixel_values.shape)\n",
    "    patchified_pixel_values = patchified_pixel_values.reshape(\n",
    "        batch_size, num_patches_one_direction * num_patches_one_direction, patch_size**2 * num_channels\n",
    "    )\n",
    "    print(patchified_pixel_values.shape)\n",
    "    return patchified_pixel_values\n",
    "\n",
    "patchified_pixel_values = patchify(pixel_values)\n",
    "print(patchified_pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"dropout\": 0.0,\n",
       "  \"hidden_act\": \"quick_gelu\",\n",
       "  \"hidden_size\": 1024,\n",
       "  \"image_size\": 336,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"model_type\": \"clip_vision_model\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"patch_size\": 14,\n",
       "  \"projection_dim\": 768,\n",
       "  \"transformers_version\": \"4.31.0\"\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionConfig\n",
    "CLIPVisionConfig.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vit_mae.configuration_vit_mae import *\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                         LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "                         \n",
    "class LlavaGeoConfig(LlamaConfig):\n",
    "    model_type: str = \"llava_geo\"\n",
    "    mae_decoder_config: ViTMAEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEConfig {\n",
       "  \"architectures\": [\n",
       "    \"ViTMAEForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 512,\n",
       "  \"decoder_intermediate_size\": 2048,\n",
       "  \"decoder_num_attention_heads\": 16,\n",
       "  \"decoder_num_hidden_layers\": 8,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mask_ratio\": 0.75,\n",
       "  \"model_type\": \"vit_mae\",\n",
       "  \"norm_pix_loss\": false,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.31.0\"\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTMAEConfig.from_pretrained(\"facebook/vit-mae-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
