{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wangz3/miniconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-25 23:13:01,956] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_geo. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ViTMAEConfig()\n",
    "# LlavaGeoConfig()\n",
    "from llava.model.language_model.llava_llama import LlavaGeoConfig, LlavaGeoLlamaForCausalLM\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from transformers.models.vit_mae.configuration_vit_mae import *\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                         LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "llava_geo_7b_config = LlavaGeoConfig.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "\n",
    "model = LlavaGeoLlamaForCausalLM(llava_geo_7b_config)\n",
    "\n",
    "vision_tower = model.get_vision_tower()\n",
    "if not vision_tower.is_loaded:\n",
    "    vision_tower.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaGeoConfig {\n",
       "  \"_name_or_path\": \"llava-v1.5-7b\",\n",
       "  \"architectures\": [\n",
       "    \"LlavaLlamaForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"freeze_mm_mlp_adapter\": false,\n",
       "  \"freeze_mm_vision_resampler\": false,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"image_aspect_ratio\": \"pad\",\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_length\": 4096,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mm_hidden_size\": 1024,\n",
       "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
       "  \"mm_resampler_type\": null,\n",
       "  \"mm_use_im_patch_token\": false,\n",
       "  \"mm_use_im_start_end\": false,\n",
       "  \"mm_vision_select_feature\": \"patch\",\n",
       "  \"mm_vision_select_layer\": -2,\n",
       "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
       "  \"model_type\": \"llava_geo\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"tune_mm_mlp_adapter\": false,\n",
       "  \"tune_mm_vision_resampler\": false,\n",
       "  \"unfreeze_mm_vision_tower\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_mm_proj\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}\n",
       ", \"mae_decoder_config\": {'base_config': 'facebook/vit-mae-base', 'norm_pix_loss': True, 'decoder_num_hidden_layers': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  -200, 29871,    13, 10994, 29892,   590, 11203,   338,   274,\n",
      "          1082]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "text_prompt = DEFAULT_IMAGE_TOKEN + '\\n' + \"Hello, my dog is cute\"\n",
    "input_ids = tokenizer_image_token(text_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select feature: cls_patch\n",
      "images.shape: torch.Size([1, 3, 336, 336])\n",
      "image_features.shape: torch.Size([1, 577, 1024])\n",
      "select feature: patch\n",
      "inputs_embeds shape torch.Size([1, 586, 4096])\n",
      "image_features_with_cls shape torch.Size([1, 577, 4096])\n",
      "sequence_unmasked shape torch.Size([1, 144, 4096])\n",
      "mask shape torch.Size([1, 576])\n",
      "ids_restore shape torch.Size([1, 576])\n",
      "reconstruction_logits shape torch.Size([1, 576, 588])\n",
      "reconstruction_loss tensor(1.1976, device='cuda:7', grad_fn=<DivBackward0>)\n",
      "semantic loss: tensor(10.9453, device='cuda:7', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "total loss: tensor(12.1429, device='cuda:7', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "device = torch.device(\"cuda:7\")\n",
    "# print(text_inputs)\n",
    "model.to(device, dtype=torch.float16)\n",
    "vision_tower.to(device=model.device, dtype=torch.float16)\n",
    "model.eval()\n",
    "# print(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"liuhaotian/llava-v1.5-7b\")\n",
    "text_prompt = DEFAULT_IMAGE_TOKEN + '\\n' + \"Hello, my dog is cute\"\n",
    "input_ids = tokenizer_image_token(\n",
    "    text_prompt, \n",
    "    tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device=model.device)\n",
    "print(input_ids.shape)\n",
    "images = torch.rand(1, 3, 336, 336).to(device=model.device, dtype=torch.float16)\n",
    "# text_inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model(input_ids, labels = input_ids, images=images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def count_parameters(model):\n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     print(f'{total_params:,} total parameters.')\n",
    "#     llava_params = sum(p.numel() for p in model.model.parameters())\n",
    "#     print(f'{llava_params:,} llava parameters.')\n",
    "#     mm_projector_params = sum(p.numel() for p in model.model.mm_projector.parameters())\n",
    "#     print(f'{mm_projector_params:,} mm projector parameters.')\n",
    "#     mae_decoder_params = sum(p.numel() for p in model.mae_decoder.parameters())\n",
    "#     print(f'{mae_decoder_params:,} mae decoder parameters.')\n",
    "\n",
    "#     print(\"mae decoder param ratio:\", mae_decoder_params / total_params)\n",
    "#     # trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "# print(model)\n",
    "# count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device, dtype=torch.float16)\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer {name} has dtype: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTMAEConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTMAEForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 4,\n",
      "  \"decoder_intermediate_size\": 8,\n",
      "  \"decoder_num_attention_heads\": 2,\n",
      "  \"decoder_num_hidden_layers\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 4,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_ratio\": 0.75,\n",
      "  \"model_type\": \"vit_mae\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "from transformers.models.vit_mae.configuration_vit_mae import *\n",
    "# from transformers.models.vit_mae.configuration_vit_mae import VitMAEConfig\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "# mae_model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
    "config = ViTMAEConfig.from_pretrained(\"facebook/vit-mae-base\")\n",
    "config.hidden_size = 4\n",
    "config.decoder_hidden_size = 4\n",
    "config.decoder_intermediate_size = 8\n",
    "config.intermediate_size = 8\n",
    "config.decoder_num_attention_heads = 2\n",
    "config.num_attention_heads = 2\n",
    "config.num_hidden_layers = 2\n",
    "config.decoder_num_hidden_layers = 2\n",
    "\n",
    "print(config)\n",
    "\n",
    "from transformers.models.vit_mae.modeling_vit_mae import *\n",
    "mae_decoder = ViTMAEDecoder(config, num_patches = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "         [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "         [0.4556, 0.6323, 0.3489, 0.4017],\n",
      "         [0.0223, 0.1689, 0.2939, 0.5185],\n",
      "         [0.6977, 0.8000, 0.1610, 0.2823],\n",
      "         [0.6816, 0.9152, 0.3971, 0.8742],\n",
      "         [0.4194, 0.5529, 0.9527, 0.0362],\n",
      "         [0.1852, 0.3734, 0.3051, 0.9320],\n",
      "         [0.1759, 0.2698, 0.1507, 0.0317],\n",
      "         [0.2081, 0.9298, 0.7231, 0.7423],\n",
      "         [0.5263, 0.2437, 0.5846, 0.0332],\n",
      "         [0.1387, 0.2422, 0.8155, 0.7932],\n",
      "         [0.2783, 0.4820, 0.8198, 0.9971],\n",
      "         [0.6984, 0.5675, 0.8352, 0.2056],\n",
      "         [0.5932, 0.1123, 0.1535, 0.2417],\n",
      "         [0.7262, 0.7011, 0.2038, 0.6511]]])\n",
      "tensor([[0.7745, 0.4369, 0.5191, 0.6159, 0.8102, 0.9801, 0.1147, 0.3168, 0.6965,\n",
      "         0.9143, 0.9351, 0.9412, 0.5995, 0.0652, 0.5460, 0.1872]])\n",
      "tensor([[13,  6, 15,  7,  1,  2, 14, 12,  3,  8,  0,  4,  9, 10, 11,  5]])\n",
      "tensor([[10,  4,  5,  8, 11, 15,  1,  3,  9, 12, 13, 14,  7,  0,  6,  2]])\n",
      "tensor([[[13, 13, 13, 13],\n",
      "         [ 6,  6,  6,  6],\n",
      "         [15, 15, 15, 15],\n",
      "         [ 7,  7,  7,  7],\n",
      "         [ 1,  1,  1,  1],\n",
      "         [ 2,  2,  2,  2],\n",
      "         [14, 14, 14, 14],\n",
      "         [12, 12, 12, 12]]])\n",
      "tensor([[[0.6984, 0.5675, 0.8352, 0.2056],\n",
      "         [0.4194, 0.5529, 0.9527, 0.0362],\n",
      "         [0.7262, 0.7011, 0.2038, 0.6511],\n",
      "         [0.1852, 0.3734, 0.3051, 0.9320],\n",
      "         [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "         [0.4556, 0.6323, 0.3489, 0.4017],\n",
      "         [0.5932, 0.1123, 0.1535, 0.2417],\n",
      "         [0.2783, 0.4820, 0.8198, 0.9971]]])\n",
      "tensor([[1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.]])\n",
      "torch.Size([1, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "def random_masking(mask_ratio, sequence, noise=None):\n",
    "    \"\"\"\n",
    "    Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random\n",
    "    noise.\n",
    "\n",
    "    Args:\n",
    "        sequence (`torch.LongTensor` of shape `(batch_size, sequence_length, dim)`)\n",
    "        noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is\n",
    "            mainly used for testing purposes to control randomness and maintain the reproducibility\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, dim = sequence.shape\n",
    "    len_keep = int(seq_length * (1 - mask_ratio))\n",
    "\n",
    "    if noise is None:\n",
    "        noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]\n",
    "    print(sequence)\n",
    "    print(noise)\n",
    "\n",
    "    # sort noise for each sample\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "    print(ids_shuffle)\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "    print(ids_restore)\n",
    "\n",
    "    # keep the first subset\n",
    "    ids_keep = ids_shuffle[:, :len_keep]\n",
    "    sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n",
    "    print(ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n",
    "    print(sequence_unmasked)\n",
    "\n",
    "    # generate the binary mask: 0 is keep, 1 is remove\n",
    "    mask = torch.ones([batch_size, seq_length], device=sequence.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    # unshuffle to get the binary mask\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "    print(mask)\n",
    "    return sequence_unmasked, mask, ids_restore\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "batch_size, num_patches, dim = 1, 16, config.hidden_size \n",
    "test_seq = torch.rand((batch_size, num_patches, dim))\n",
    "sequence_unmasked, mask, ids_restore = random_masking(0.5, test_seq)\n",
    "# add cls token\n",
    "cls_token = torch.rand((batch_size, 1, dim))\n",
    "hidden_states = torch.cat((cls_token, sequence_unmasked), dim=1)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 4])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "decoder_output = mae_decoder(hidden_states, ids_restore)\n",
    "print(hidden_states.shape)\n",
    "print(ids_restore.shape)\n",
    "print(decoder_output.logits.shape) # (batch_size, num_patches, patch_size * patch_size * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"clip_vision_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 32,\n",
      "  \"projection_dim\": 512,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "CLIPVisionEmbeddings(\n",
      "  (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "  (position_embedding): Embedding(50, 768)\n",
      ")\n",
      "patch_embeds.shape torch.Size([1, 768, 7, 7])\n",
      "patch_embeds.shape falttened torch.Size([1, 49, 768])\n",
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# the following is how CLIP encoder get the image patches\n",
    "# the patches are ordered: left to right, top to bottom\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionConfig\n",
    "class CLIPVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches + 1\n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        target_dtype = self.patch_embedding.weight.dtype\n",
    "        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n",
    "        print(\"patch_embeds.shape\", patch_embeds.shape)\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n",
    "        print(\"patch_embeds.shape falttened\", patch_embeds.shape)\n",
    "\n",
    "        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n",
    "        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        return embeddings\n",
    "\n",
    "clip_config = CLIPVisionConfig()\n",
    "print(clip_config)\n",
    "clip_emb_layer = CLIPVisionEmbeddings(clip_config)\n",
    "print(clip_emb_layer)\n",
    "pixel_values = torch.rand((batch_size, 3, 224, 224))\n",
    "patch_embeds = clip_emb_layer(pixel_values)\n",
    "print(patch_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([1, 3, 7, 32, 7, 32])\n",
      "torch.Size([1, 7, 7, 32, 32, 3])\n",
      "torch.Size([1, 49, 3072])\n",
      "torch.Size([1, 49, 3072])\n"
     ]
    }
   ],
   "source": [
    "patch_size, num_channels = 32, 3\n",
    "def patchify(pixel_values):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Pixel values.\n",
    "\n",
    "    Returns:\n",
    "        `torch.FloatTensor` of shape `(batch_size, num_patches, patch_size**2 * num_channels)`:\n",
    "            Patchified pixel values.\n",
    "    \"\"\"\n",
    "    # sanity checks\n",
    "    if (pixel_values.shape[2] != pixel_values.shape[3]) or (pixel_values.shape[2] % patch_size != 0):\n",
    "        raise ValueError(\"Make sure the pixel values have a squared size that is divisible by the patch size\")\n",
    "    if pixel_values.shape[1] != num_channels:\n",
    "        raise ValueError(\n",
    "            \"Make sure the number of channels of the pixel values is equal to the one set in the configuration\"\n",
    "        )\n",
    "\n",
    "    # patchify\n",
    "    batch_size = pixel_values.shape[0]\n",
    "    num_patches_one_direction = pixel_values.shape[2] // patch_size\n",
    "    print(num_patches_one_direction)\n",
    "    patchified_pixel_values = pixel_values.reshape(\n",
    "        batch_size, num_channels, num_patches_one_direction, patch_size, num_patches_one_direction, patch_size\n",
    "    )\n",
    "    print(patchified_pixel_values.shape)\n",
    "    patchified_pixel_values = torch.einsum(\"nchpwq->nhwpqc\", patchified_pixel_values)\n",
    "    print(patchified_pixel_values.shape)\n",
    "    patchified_pixel_values = patchified_pixel_values.reshape(\n",
    "        batch_size, num_patches_one_direction * num_patches_one_direction, patch_size**2 * num_channels\n",
    "    )\n",
    "    print(patchified_pixel_values.shape)\n",
    "    return patchified_pixel_values\n",
    "\n",
    "patchified_pixel_values = patchify(pixel_values)\n",
    "print(patchified_pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"dropout\": 0.0,\n",
       "  \"hidden_act\": \"quick_gelu\",\n",
       "  \"hidden_size\": 1024,\n",
       "  \"image_size\": 336,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"model_type\": \"clip_vision_model\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"patch_size\": 14,\n",
       "  \"projection_dim\": 768,\n",
       "  \"transformers_version\": \"4.31.0\"\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionConfig\n",
    "CLIPVisionConfig.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vit_mae.configuration_vit_mae import *\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                         LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "                         \n",
    "class LlavaGeoConfig(LlamaConfig):\n",
    "    model_type: str = \"llava_geo\"\n",
    "    mae_decoder_config: ViTMAEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEConfig {\n",
       "  \"architectures\": [\n",
       "    \"ViTMAEForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 512,\n",
       "  \"decoder_intermediate_size\": 2048,\n",
       "  \"decoder_num_attention_heads\": 16,\n",
       "  \"decoder_num_hidden_layers\": 8,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mask_ratio\": 0.75,\n",
       "  \"model_type\": \"vit_mae\",\n",
       "  \"norm_pix_loss\": false,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.31.0\"\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTMAEConfig.from_pretrained(\"facebook/vit-mae-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
